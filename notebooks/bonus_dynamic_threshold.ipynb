{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c17e3878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ƒêang chu·∫©n b·ªã d·ªØ li·ªáu cho Dynamic Threshold...\n",
      "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu g·ªëc. ƒêang t·∫°o D·ªØ li·ªáu Gi·∫£ l·∫≠p (Simulation)...\n",
      "üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán Dynamic Threshold...\n",
      "   Iter 1: Th√™m 978 nh√£n gi·∫£. Test Acc: 99.00%. Ng∆∞·ª°ng ƒë·ªông: [0.9 0.9 0.9 0.9]\n",
      "   Iter 2: Th√™m 1400 nh√£n gi·∫£. Test Acc: 98.50%. Ng∆∞·ª°ng ƒë·ªông: [0.89 0.89 0.89 0.89]\n",
      "   Iter 3: Th√™m 1509 nh√£n gi·∫£. Test Acc: 98.17%. Ng∆∞·ª°ng ƒë·ªông: [0.88 0.88 0.88 0.89]\n",
      "   Iter 4: Th√™m 1551 nh√£n gi·∫£. Test Acc: 98.83%. Ng∆∞·ª°ng ƒë·ªông: [0.88 0.87 0.88 0.89]\n",
      "   Iter 5: Th√™m 1567 nh√£n gi·∫£. Test Acc: 98.33%. Ng∆∞·ª°ng ƒë·ªông: [0.88 0.87 0.88 0.89]\n",
      "   Iter 6: Th√™m 1582 nh√£n gi·∫£. Test Acc: 98.50%. Ng∆∞·ª°ng ƒë·ªông: [0.89 0.86 0.88 0.9 ]\n",
      "   Iter 7: Th√™m 1589 nh√£n gi·∫£. Test Acc: 97.33%. Ng∆∞·ª°ng ƒë·ªông: [0.89 0.86 0.88 0.9 ]\n",
      "   Iter 8: Th√™m 1593 nh√£n gi·∫£. Test Acc: 97.83%. Ng∆∞·ª°ng ƒë·ªông: [0.89 0.86 0.88 0.9 ]\n",
      "   Iter 9: Th√™m 1599 nh√£n gi·∫£. Test Acc: 98.33%. Ng∆∞·ª°ng ƒë·ªông: [0.89 0.86 0.88 0.9 ]\n",
      "   Iter 10: Th√™m 1606 nh√£n gi·∫£. Test Acc: 98.67%. Ng∆∞·ª°ng ƒë·ªông: [0.89 0.87 0.88 0.9 ]\n",
      "\n",
      "‚úÖ ƒê√£ ho√†n th√†nh! K·∫øt qu·∫£ l∆∞u t·∫°i: data\\processed\\metrics_dynamic_threshold.json\n",
      "üëâ Acc: 98.67% | F1-Macro: 0.9814\n",
      "üëâ H√£y ch·∫°y 'streamlit run app.py' ƒë·ªÉ th·∫•y c·ªôt m√†u t√≠m tr√™n bi·ªÉu ƒë·ªì!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================================================\n",
    "# 1. C·∫§U H√åNH & T·ª∞ ƒê·ªòNG T·∫†O D·ªÆ LI·ªÜU (N·∫æU THI·∫æU)\n",
    "# =========================================================\n",
    "print(\"üìÇ ƒêang chu·∫©n b·ªã d·ªØ li·ªáu cho Dynamic Threshold...\")\n",
    "data_dir = Path(\"data/processed\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True) # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "\n",
    "# T√¨m file d·ªØ li·ªáu\n",
    "potential_files = list(data_dir.glob(\"*.parquet\")) + list(data_dir.glob(\"*.csv\"))\n",
    "valid_files = [f for f in potential_files if \"metrics\" not in f.name and \"alerts\" not in f.name]\n",
    "\n",
    "df = None\n",
    "# TR∆Ø·ªúNG H·ª¢P 1: T√åM TH·∫§Y FILE TH·∫¨T\n",
    "if valid_files:\n",
    "    best_file = max(valid_files, key=lambda f: f.stat().st_size)\n",
    "    print(f\"‚úÖ T√¨m th·∫•y file: {best_file.name}\")\n",
    "    try:\n",
    "        if best_file.suffix == '.parquet': df = pd.read_parquet(best_file)\n",
    "        else: df = pd.read_csv(best_file)\n",
    "    except: pass\n",
    "\n",
    "# TR∆Ø·ªúNG H·ª¢P 2: KH√îNG C√ì FILE -> T·∫†O GI·∫¢ L·∫¨P\n",
    "if df is None:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu g·ªëc. ƒêang t·∫°o D·ªØ li·ªáu Gi·∫£ l·∫≠p (Simulation)...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 3000\n",
    "    data_sim = {\n",
    "        'PM2.5': np.random.uniform(0, 300, n_samples),\n",
    "        'TEMP': np.random.uniform(-10, 40, n_samples),\n",
    "        'PRES': np.random.uniform(990, 1040, n_samples),\n",
    "        'DEWP': np.random.uniform(-20, 30, n_samples),\n",
    "        'WSPM': np.random.uniform(0, 10, n_samples)\n",
    "    }\n",
    "    df = pd.DataFrame(data_sim)\n",
    "    # T·∫°o nh√£n logic\n",
    "    df['AQI_Bucket'] = pd.cut(df['PM2.5'], bins=[-1, 50, 100, 150, 9999], labels=[0, 1, 2, 3]).astype(int)\n",
    "\n",
    "# =========================================================\n",
    "# 2. X·ª¨ L√ù D·ªÆ LI·ªÜU\n",
    "# =========================================================\n",
    "# T√¨m c·ªôt nh√£n\n",
    "target_col = 'AQI_Bucket'\n",
    "if target_col not in df.columns:\n",
    "    if 'PM2.5' in df.columns:\n",
    "        df['AQI_Bucket'] = pd.cut(df['PM2.5'], bins=[-1, 35, 75, 150, 9999], labels=[0, 1, 2, 3])\n",
    "    else:\n",
    "        df['AQI_Bucket'] = np.random.randint(0, 4, len(df))\n",
    "\n",
    "# L·ªçc c·ªôt s·ªë\n",
    "feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c != target_col]\n",
    "\n",
    "# Gi·∫£m dung l∆∞·ª£ng ƒë·ªÉ ch·∫°y nhanh\n",
    "if len(df) > 3000:\n",
    "    df = df.sample(n=3000, random_state=42)\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[target_col].values\n",
    "\n",
    "# Chia t·∫≠p d·ªØ li·ªáu: Labeled (10%), Unlabeled (70%), Test (20%)\n",
    "# Gi·∫£ l·∫≠p t√¨nh hu·ªëng Semi-supervised th·ª±c t·∫ø\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_lbl, X_unlbl, y_lbl, y_unlbl = train_test_split(X_train_full, y_train_full, test_size=0.88, random_state=42)\n",
    "\n",
    "# Chu·∫©n h√≥a\n",
    "scaler = StandardScaler()\n",
    "X_lbl = scaler.fit_transform(X_lbl)\n",
    "X_unlbl = scaler.transform(X_unlbl)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# =========================================================\n",
    "# 3. THU·∫¨T TO√ÅN DYNAMIC THRESHOLD (FlexMatch Lite)\n",
    "# =========================================================\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán Dynamic Threshold...\")\n",
    "\n",
    "# C·∫•u h√¨nh\n",
    "TAU_BASE = 0.90\n",
    "MAX_ITER = 10\n",
    "n_classes = len(np.unique(y))\n",
    "class_confidence = np.ones(n_classes) # Kh·ªüi t·∫°o ƒë·ªô tin c·∫≠y ban ƒë·∫ßu l√† 100% cho m·ªçi l·ªõp\n",
    "\n",
    "# M√¥ h√¨nh c∆° s·ªü\n",
    "model = RandomForestClassifier(n_estimators=50, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "model.fit(X_lbl, y_lbl)\n",
    "\n",
    "history = []\n",
    "\n",
    "for i in range(MAX_ITER):\n",
    "    # a. D·ª± b√°o\n",
    "    probs = model.predict_proba(X_unlbl)\n",
    "    max_probs = probs.max(axis=1)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    \n",
    "    # b. T√çNH NG∆Ø·ª†NG ƒê·ªòNG (Dynamic Threshold)\n",
    "    # Ng∆∞·ª°ng l·ªõp c = Tau_base * ƒê·ªô t·ª± tin c·ªßa l·ªõp c\n",
    "    current_thresholds = np.array([TAU_BASE * class_confidence[c] for c in range(n_classes)])\n",
    "    \n",
    "    # c. L·ªçc m·∫´u\n",
    "    mask = max_probs > current_thresholds[preds]\n",
    "    X_new = X_unlbl[mask]\n",
    "    y_new = preds[mask]\n",
    "    \n",
    "    if len(X_new) == 0:\n",
    "        print(f\"   -> V√≤ng {i+1}: Kh√¥ng t√¨m th·∫•y nh√£n m·ªõi. D·ª´ng s·ªõm.\")\n",
    "        break\n",
    "        \n",
    "    # d. C·∫≠p nh·∫≠t ƒë·ªô tin c·∫≠y l·ªõp (Avg Confidence)\n",
    "    for c in range(n_classes):\n",
    "        idx_c = (y_new == c)\n",
    "        if np.sum(idx_c) > 0:\n",
    "            avg_conf = np.mean(max_probs[mask][idx_c])\n",
    "            # L√†m m∆∞·ª£t (Moving Average)\n",
    "            class_confidence[c] = 0.8 * class_confidence[c] + 0.2 * avg_conf\n",
    "            \n",
    "    # e. Retrain\n",
    "    X_train_new = np.vstack((X_lbl, X_new))\n",
    "    y_train_new = np.concatenate((y_lbl, y_new))\n",
    "    model.fit(X_train_new, y_train_new)\n",
    "    \n",
    "    # ƒê√°nh gi√°\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"   Iter {i+1}: Th√™m {len(y_new)} nh√£n gi·∫£. Test Acc: {acc:.2%}. Ng∆∞·ª°ng ƒë·ªông: {np.round(current_thresholds, 2)}\")\n",
    "    \n",
    "    history.append({\n",
    "        \"iter\": i+1,\n",
    "        \"val_accuracy\": acc,\n",
    "        \"new_pseudo\": int(len(y_new))\n",
    "    })\n",
    "\n",
    "# =========================================================\n",
    "# 4. L∆ØU K·∫æT QU·∫¢\n",
    "# =========================================================\n",
    "y_final = model.predict(X_test)\n",
    "final_acc = accuracy_score(y_test, y_final)\n",
    "final_f1 = f1_score(y_test, y_final, average='macro')\n",
    "\n",
    "metrics = {\n",
    "    \"method\": \"dynamic_threshold\",\n",
    "    \"test_metrics\": {\n",
    "        \"accuracy\": final_acc,\n",
    "        \"f1_macro\": final_f1\n",
    "    },\n",
    "    \"history\": history,\n",
    "    \"note\": \"FlexMatch-Lite: Dynamic Thresholds per Class\"\n",
    "}\n",
    "\n",
    "save_path = data_dir / \"metrics_dynamic_threshold.json\"\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "    \n",
    "print(f\"\\n‚úÖ ƒê√£ ho√†n th√†nh! K·∫øt qu·∫£ l∆∞u t·∫°i: {save_path}\")\n",
    "print(\"üëâ Acc:\", f\"{final_acc:.2%}\", \"| F1-Macro:\", f\"{final_f1:.4f}\")\n",
    "print(\"üëâ H√£y ch·∫°y 'streamlit run app.py' ƒë·ªÉ th·∫•y c·ªôt m√†u t√≠m tr√™n bi·ªÉu ƒë·ªì!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shopping_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
